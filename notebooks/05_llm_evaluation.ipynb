{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dec89c7",
   "metadata": {},
   "source": [
    "# LLM Evaluation & Prompt Testing\n",
    "\n",
    "## Current System Overview\n",
    "\n",
    "### RAG Pipeline Architecture\n",
    "\n",
    "**Purpose**: An AI civics tutor that evaluates user answers for the USCIS citizenship test and provides educational feedback.\n",
    "\n",
    "**Components**:\n",
    "1. **Question Bank**: Official USCIS civics test questions with up-to-date correct answers\n",
    "2. **Vector Database**: Qdrant containing embedded USCIS Civics Guide passages\n",
    "3. **Retrieval**: Semantic search retrieves relevant context from civics guide\n",
    "4. **LLM Evaluation**: OpenAI model evaluates answer and generates educational feedback\n",
    "\n",
    "**System Inputs**:\n",
    "- **Question**: The civics test question being asked\n",
    "- **Correct Answers**: Up-to-date acceptable answers (updated monthly via automated ingestion)\n",
    "- **User Answer**: The student's submitted answer\n",
    "- **Context**: Retrieved passages from USCIS Civics Guide (2-4 chunks via RAG)\n",
    "- **User State**: Student's location for state-specific questions (governor, senators, etc.)\n",
    "\n",
    "**System Outputs**:\n",
    "The bot returns a JSON response with three fields:\n",
    "\n",
    "1. **`success`** (boolean): Pass/fail evaluation of the user's answer\n",
    "2. **`reason`** (string): Explanation for why the answer passed or failed\n",
    "3. **`background_info`** (string): Educational context to help student learn and retain information\n",
    "\n",
    "**Optimal RAG Configuration** (from the retrieval evaluation analysis (here)[./04_retrieval_evaluation.ipynb]):\n",
    "- Context limit: 4 chunks\n",
    "- success threshold: 0.3\n",
    "- Query expansion: False\n",
    "- Temperature: 0.5\n",
    "\n",
    "---\n",
    "\n",
    "## Current Problems Identified\n",
    "\n",
    "### Problem 1: LLM Using Outdated Training Data Instead of Provided Answers\n",
    "**Description**: The LLM sometimes relies on its training data instead of the correct answers explicitly provided in the prompt.\n",
    "\n",
    "**Critical Example**:\n",
    "- **Question**: \"Who is the current US President?\"\n",
    "- **Provided correct answer**: \"Donald Trump\" (from monthly-updated data ingestion)\n",
    "- **User answer**: \"Donald Trump\"\n",
    "- **LLM behavior**: Marks answer as **INCORRECT** because LLM was trained on 2024 data when Biden was president\n",
    "- **Root cause**: LLM ignoring the `answers` parameter and using its outdated world knowledge\n",
    "\n",
    "**Impact**: \n",
    "- System gives wrong evaluations for time-sensitive questions (presidents, governors, senators, representatives)\n",
    "- Defeats the purpose of monthly automated data updates\n",
    "- Users get marked wrong when they're actually correct\n",
    "\n",
    "**Affected question types**:\n",
    "- Current officeholders (president, VP, governors, senators, representatives)\n",
    "- Any time-sensitive civics information that changes\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2: Grading Too Harsh\n",
    "**Description**: The evaluation is overly strict and marks semantically correct answers as wrong.\n",
    "\n",
    "**Examples**:\n",
    "- ‚ùå User makes minor **typos** but answer is semantically correct ‚Üí marked as fail\n",
    "- ‚ùå Question asks for **2 answers**, user provides **3 answers** where 2 are correct ‚Üí marked as fail  \n",
    "- ‚ùå User answer is **semantically equivalent** but doesn't use exact wording ‚Üí marked as fail\n",
    "\n",
    "**Example**:\n",
    "- Correct answer: \"freedom of speech\"\n",
    "- User answer: \"fredom of speach\" (typos) ‚Üí marked wrong\n",
    "- User answer: \"right to free speech\" (equivalent) ‚Üí marked wrong\n",
    "\n",
    "**Impact**: Students get discouraged when correct answers are marked wrong due to technicalities.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 3: Background Info Too Short/Generic\n",
    "**Description**: The background information often lacks educational depth and substance.\n",
    "\n",
    "**Symptoms**:\n",
    "- Background info is only 1-2 sentences (too short to be educational)\n",
    "- Information appears generic and doesn't leverage the retrieved context\n",
    "- Students report feeling like they \"learned nothing\" from the background\n",
    "\n",
    "**Current prompt guidance**: \"Keep background info short\" (likely contributing to this issue)\n",
    "\n",
    "**Impact**: Defeats the learning purpose - students don't retain information beyond rote memorization.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 4: Poor Context Utilization\n",
    "**Description**: Unclear whether the LLM actually uses the retrieved RAG context to generate background info.\n",
    "\n",
    "**Observation**: Some background info appears generic enough that it could have been generated without any context retrieval.\n",
    "\n",
    "**Impact**: \n",
    "- Wastes the RAG pipeline effort\n",
    "- Missing opportunity to provide authoritative, context-grounded education\n",
    "- Background info lacks depth and specificity\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 5: Redundancy Between Reason & Background\n",
    "**Description**: The `reason` and `background_info` fields often contain overlapping or repetitive information.\n",
    "\n",
    "**Example**:\n",
    "```json\n",
    "{\n",
    "  \"reason\": \"Your answer is correct. The three branches of government are legislative, executive, and judicial.\",\n",
    "  \"background_info\": \"The U.S. government has three branches: legislative, executive, and judicial. This separation ensures checks and balances.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Impact**: \n",
    "- Wastes tokens and API costs\n",
    "- Poor user experience (feels repetitive)\n",
    "- Background info doesn't add value beyond the reason\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Goals\n",
    "\n",
    "### Primary Objectives\n",
    "\n",
    "1. **Fix Training Data Override Issue** üî¥ **CRITICAL**\n",
    "   - Ensure LLM strictly uses provided `answers` parameter, not its training data\n",
    "   - Test with time-sensitive questions (current president, governors, etc.)\n",
    "   - Success metric: 100% accuracy on questions with outdated training data\n",
    "\n",
    "2. **Reduce Harsh Grading**\n",
    "   - Accept answers with minor typos (high edit distance similarity)\n",
    "   - Accept semantically equivalent answers (same meaning, different words)\n",
    "   - Pass answers for multi-part questions when some parts are correct and satisfied the min number of given correct values in list\n",
    "   - Success metric: bot passing answers where 3 given, 2 min needed, and 2/3 were correct. Measured via llm-as-judge grading accuracy\n",
    "\n",
    "3. **Improve Background Info Quality**\n",
    "   - Increase substantive length (target: 50-80 words minimum)\n",
    "   - Ensure educational value and learning retention\n",
    "   - Success metric: Average word count ‚â•50, higher LLM-as-judge successs\n",
    "\n",
    "4. **Ensure Context Usage**\n",
    "   - Background info must demonstrably use retrieved RAG context\n",
    "   - Include specific facts/details from context passages\n",
    "   - Success metric: High context overlap ratio, LLM-as-judge confirms context usage\n",
    "\n",
    "5. **Eliminate Redundancy**\n",
    "   - Make `reason` and `background_info` distinct and complementary\n",
    "   - `reason` = explain grading decision\n",
    "   - `background_info` = teach something new beyond grading\n",
    "   - Success metric: Reason-background similarity success <0.5\n",
    "\n",
    "### Success Criteria Summary\n",
    "We are going to explore the following metrics and see their trends given changes in prompt\n",
    "#### Quantitative metrics\n",
    "- positive feedback rate\n",
    "- background word count\n",
    "- context usage in background\n",
    "- reason-background similarity\n",
    "#### Qualitative metrics (LLM-as-judge)\n",
    "- Override of context data\n",
    "- Grading accuracy\n",
    "- Background info quality\n",
    "- Overall learning helpfulness\n",
    "\n",
    "\n",
    "## Procedure\n",
    "We will focus right now on the 2008 test, for arizona. For two reasons: I am from AZ, and the 2008 test is the one all people are taking right now since oct 20 has not yet passed.\n",
    "\n",
    "We will:\n",
    "1. Try out the current exam setup ~50 times to create a golden dataset with actual feedback.\n",
    "2. check out the basic quantitative metrics to see how we're doing... and if we indeed are seeing consistently some of the issues mentioned above\n",
    "3. Try out LLM-as-judge to check the 3 qualitativem metrics that should give us insight into our overall performance\n",
    "4. Log our current 'base' successs\n",
    "5. Try out some prompt changes\n",
    "6. see if our metrics improve/worsen\n",
    "7. settle on the best prompt out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea771e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4870a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Neon!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the database URL\n",
    "DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "\n",
    "# Connect to Neon\n",
    "conn = psycopg2.connect(DATABASE_URL)\n",
    "print(\"‚úÖ Connected to Neon!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fbc5998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bq/bvtwklb14vs146h6cdng_jvm0000gq/T/ipykernel_63191/2171077951.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feedback entries: 70\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_state</th>\n",
       "      <th>test_year</th>\n",
       "      <th>question_text</th>\n",
       "      <th>correct_answers</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>success</th>\n",
       "      <th>reason</th>\n",
       "      <th>background_info</th>\n",
       "      <th>feedback_type</th>\n",
       "      <th>session_id</th>\n",
       "      <th>rag_context_limit</th>\n",
       "      <th>rag_score_threshold</th>\n",
       "      <th>rag_query_expansion</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>model</th>\n",
       "      <th>llm_temperature</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>2025-10-16 23:12:25.165481</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2008</td>\n",
       "      <td>What does the President‚Äôs Cabinet do?</td>\n",
       "      <td>{\"advises the President\"}</td>\n",
       "      <td>they advice the president on their area of exp...</td>\n",
       "      <td>False</td>\n",
       "      <td>Almost there! The correct answer is that the P...</td>\n",
       "      <td>The President‚Äôs Cabinet is like a secret sauce...</td>\n",
       "      <td>positive</td>\n",
       "      <td>08eee2c4-7a3c-434d-b7d8-b219e6acef5d</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>\\nYou are a friendly USCIS officer helping the...</td>\n",
       "      <td>\\n&lt;question&gt;\\nWhat does the President‚Äôs Cabine...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Page 13:\\nTHE EXECUTIVE BRANCH\\nIn this chapte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>2025-10-16 23:12:01.025205</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2008</td>\n",
       "      <td>Who does a U.S. Senator represent?</td>\n",
       "      <td>{\"all people of the state\"}</td>\n",
       "      <td>their state</td>\n",
       "      <td>False</td>\n",
       "      <td>Close, but not quite! A U.S. Senator represent...</td>\n",
       "      <td>Each state has two U.S. Senators, so it‚Äôs like...</td>\n",
       "      <td>positive</td>\n",
       "      <td>08eee2c4-7a3c-434d-b7d8-b219e6acef5d</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>\\nYou are a friendly USCIS officer helping the...</td>\n",
       "      <td>\\n&lt;question&gt;\\nWho does a U.S. Senator represen...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Page 11:\\nElecting Members of the House of Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>2025-10-16 23:10:41.910605</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2008</td>\n",
       "      <td>Why do some states have more Representatives t...</td>\n",
       "      <td>{\"(because of) the state‚Äô s population\",\"(beca...</td>\n",
       "      <td>some states they got more people than othersta...</td>\n",
       "      <td>False</td>\n",
       "      <td>Close, but not quite! The correct answer is th...</td>\n",
       "      <td>Did you know that California has 52 representa...</td>\n",
       "      <td>negative</td>\n",
       "      <td>08eee2c4-7a3c-434d-b7d8-b219e6acef5d</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>\\nYou are a friendly USCIS officer helping the...</td>\n",
       "      <td>\\n&lt;question&gt;\\nWhy do some states have more Rep...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Page 10:\\nTHE LEGISLATIVE BRANCH (CONGRESS)\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>2025-10-16 23:10:05.985411</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2008</td>\n",
       "      <td>What is the economic system in the United States?</td>\n",
       "      <td>{\"capitalist economy\",\"market economy\"}</td>\n",
       "      <td>capitalism economy</td>\n",
       "      <td>False</td>\n",
       "      <td>Close, but not quite! The correct terms are 'c...</td>\n",
       "      <td>In a market economy, the government doesn‚Äôt co...</td>\n",
       "      <td>negative</td>\n",
       "      <td>08eee2c4-7a3c-434d-b7d8-b219e6acef5d</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>\\nYou are a friendly USCIS officer helping the...</td>\n",
       "      <td>\\n&lt;question&gt;\\nWhat is the economic system in t...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Page 32:\\nAMERICAN HISTORY: 1900-2001\\nIn this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>2025-10-16 23:09:46.921651</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2008</td>\n",
       "      <td>When is the last day you can send in federal i...</td>\n",
       "      <td>{\"April 15\"}</td>\n",
       "      <td>apr 15</td>\n",
       "      <td>True</td>\n",
       "      <td>Correct! The last day to send in federal incom...</td>\n",
       "      <td>If April 15 falls on a weekend or holiday, the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>08eee2c4-7a3c-434d-b7d8-b219e6acef5d</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>\\nYou are a friendly USCIS officer helping the...</td>\n",
       "      <td>\\n&lt;question&gt;\\nWhen is the last day you can sen...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.5</td>\n",
       "      <td>No relevant context found.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                  timestamp user_state test_year  \\\n",
       "0  70 2025-10-16 23:12:25.165481    Arizona      2008   \n",
       "1  69 2025-10-16 23:12:01.025205    Arizona      2008   \n",
       "2  68 2025-10-16 23:10:41.910605    Arizona      2008   \n",
       "3  67 2025-10-16 23:10:05.985411    Arizona      2008   \n",
       "4  66 2025-10-16 23:09:46.921651    Arizona      2008   \n",
       "\n",
       "                                       question_text  \\\n",
       "0              What does the President‚Äôs Cabinet do?   \n",
       "1                 Who does a U.S. Senator represent?   \n",
       "2  Why do some states have more Representatives t...   \n",
       "3  What is the economic system in the United States?   \n",
       "4  When is the last day you can send in federal i...   \n",
       "\n",
       "                                     correct_answers  \\\n",
       "0                          {\"advises the President\"}   \n",
       "1                        {\"all people of the state\"}   \n",
       "2  {\"(because of) the state‚Äô s population\",\"(beca...   \n",
       "3            {\"capitalist economy\",\"market economy\"}   \n",
       "4                                       {\"April 15\"}   \n",
       "\n",
       "                                         user_answer  success  \\\n",
       "0  they advice the president on their area of exp...    False   \n",
       "1                                        their state    False   \n",
       "2  some states they got more people than othersta...    False   \n",
       "3                                 capitalism economy    False   \n",
       "4                                             apr 15     True   \n",
       "\n",
       "                                              reason  \\\n",
       "0  Almost there! The correct answer is that the P...   \n",
       "1  Close, but not quite! A U.S. Senator represent...   \n",
       "2  Close, but not quite! The correct answer is th...   \n",
       "3  Close, but not quite! The correct terms are 'c...   \n",
       "4  Correct! The last day to send in federal incom...   \n",
       "\n",
       "                                     background_info feedback_type  \\\n",
       "0  The President‚Äôs Cabinet is like a secret sauce...      positive   \n",
       "1  Each state has two U.S. Senators, so it‚Äôs like...      positive   \n",
       "2  Did you know that California has 52 representa...      negative   \n",
       "3  In a market economy, the government doesn‚Äôt co...      negative   \n",
       "4  If April 15 falls on a weekend or holiday, the...      positive   \n",
       "\n",
       "                             session_id  rag_context_limit  \\\n",
       "0  08eee2c4-7a3c-434d-b7d8-b219e6acef5d                  4   \n",
       "1  08eee2c4-7a3c-434d-b7d8-b219e6acef5d                  4   \n",
       "2  08eee2c4-7a3c-434d-b7d8-b219e6acef5d                  4   \n",
       "3  08eee2c4-7a3c-434d-b7d8-b219e6acef5d                  4   \n",
       "4  08eee2c4-7a3c-434d-b7d8-b219e6acef5d                  4   \n",
       "\n",
       "   rag_score_threshold  rag_query_expansion  \\\n",
       "0                  0.3                False   \n",
       "1                  0.3                False   \n",
       "2                  0.3                False   \n",
       "3                  0.3                False   \n",
       "4                  0.3                False   \n",
       "\n",
       "                                       system_prompt  \\\n",
       "0  \\nYou are a friendly USCIS officer helping the...   \n",
       "1  \\nYou are a friendly USCIS officer helping the...   \n",
       "2  \\nYou are a friendly USCIS officer helping the...   \n",
       "3  \\nYou are a friendly USCIS officer helping the...   \n",
       "4  \\nYou are a friendly USCIS officer helping the...   \n",
       "\n",
       "                                         user_prompt        model  \\\n",
       "0  \\n<question>\\nWhat does the President‚Äôs Cabine...  gpt-4o-mini   \n",
       "1  \\n<question>\\nWho does a U.S. Senator represen...  gpt-4o-mini   \n",
       "2  \\n<question>\\nWhy do some states have more Rep...  gpt-4o-mini   \n",
       "3  \\n<question>\\nWhat is the economic system in t...  gpt-4o-mini   \n",
       "4  \\n<question>\\nWhen is the last day you can sen...  gpt-4o-mini   \n",
       "\n",
       "   llm_temperature                                            context  \n",
       "0              0.5  Page 13:\\nTHE EXECUTIVE BRANCH\\nIn this chapte...  \n",
       "1              0.5  Page 11:\\nElecting Members of the House of Rep...  \n",
       "2              0.5  Page 10:\\nTHE LEGISLATIVE BRANCH (CONGRESS)\\nI...  \n",
       "3              0.5  Page 32:\\nAMERICAN HISTORY: 1900-2001\\nIn this...  \n",
       "4              0.5                         No relevant context found.  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load feedback data\n",
    "df = pd.read_sql(\"\"\"\n",
    "    SELECT * FROM feedback \n",
    "    ORDER BY timestamp DESC\n",
    "\"\"\", conn)\n",
    "\n",
    "print(f\"Total feedback entries: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf608a",
   "metadata": {},
   "source": [
    "## Quantitative metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f928794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get positive feedback rate\n",
    "def get_positive_feedback_rate(df):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of positive feedback.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'feedback_type' column containing 'positive' or 'negative'\n",
    "    \n",
    "    Returns:\n",
    "        float: Percentage of positive feedback (0-100)\n",
    "    \"\"\"\n",
    "    positive_count = (df['feedback_type'] == 'positive').sum()\n",
    "    total_count = len(df)\n",
    "    \n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (positive_count / total_count) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "029f5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_background_word_count(df):\n",
    "    \"\"\"\n",
    "    Add background_word_count column to DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'background_info' column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Original DataFrame with new 'background_info_word_count' column added\n",
    "    \"\"\"\n",
    "    df['background_info_word_count'] = df['background_info'].str.split().str.len()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "442ea5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_context_usage(df):\n",
    "    \"\"\"\n",
    "    Calculate what percentage of context words appear in background_info.\n",
    "    \n",
    "    Returns:\n",
    "        Series: Context usage ratio for each row (0-1)\n",
    "    \"\"\"\n",
    "    def word_overlap_ratio(row):\n",
    "        # Get words from context and background\n",
    "        context_words = set(str(row['context']).lower().split())\n",
    "        background_words = set(str(row['background_info']).lower().split())\n",
    "        \n",
    "        # Remove common stop words to focus on meaningful overlap\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'is', 'are', 'was', 'were', 'page'}\n",
    "        context_words -= stop_words\n",
    "        background_words -= stop_words\n",
    "        \n",
    "        # Calculate overlap\n",
    "        if len(context_words) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(context_words & background_words)\n",
    "        return overlap / len(context_words)\n",
    "    \n",
    "    return df.apply(word_overlap_ratio, axis=1)\n",
    "\n",
    "def calculate_ngram_overlap(df, n=3):\n",
    "    \"\"\"\n",
    "    Calculate overlap of n-grams between context and background.\n",
    "    Better at catching actual usage vs coincidental word matches.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    def get_ngrams(text, n):\n",
    "        words = text.lower().split()\n",
    "        return set([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n",
    "    \n",
    "    def ngram_overlap_ratio(row):\n",
    "        context_ngrams = get_ngrams(str(row['context']), n)\n",
    "        background_ngrams = get_ngrams(str(row['background_info']), n)\n",
    "        \n",
    "        if len(context_ngrams) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(context_ngrams & background_ngrams)\n",
    "        return overlap / len(context_ngrams)\n",
    "    \n",
    "    return df.apply(ngram_overlap_ratio, axis=1)\n",
    "\n",
    "def calculate_key_term_usage(df):\n",
    "    \"\"\"\n",
    "    Check if key terms from context appear in background.\n",
    "    Focuses on proper nouns, numbers, and capitalized terms.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    def extract_key_terms(text):\n",
    "        # Find capitalized words (likely proper nouns)\n",
    "        capitalized = set(re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text))\n",
    "        # Find numbers\n",
    "        numbers = set(re.findall(r'\\b\\d+\\b', text))\n",
    "        # Find words in quotes\n",
    "        quoted = set(re.findall(r'\"([^\"]*)\"', text))\n",
    "        \n",
    "        return capitalized | numbers | quoted\n",
    "    \n",
    "    def key_term_ratio(row):\n",
    "        context_terms = extract_key_terms(str(row['context']))\n",
    "        background_text = str(row['background_info']).lower()\n",
    "        \n",
    "        if len(context_terms) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Check how many key terms appear in background\n",
    "        found = sum(1 for term in context_terms if term.lower() in background_text)\n",
    "        return found / len(context_terms)\n",
    "    \n",
    "    return df.apply(key_term_ratio, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8dd0a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_context_usage(df):\n",
    "    \"\"\"\n",
    "    Comprehensive context usage analysis using multiple metrics.\n",
    "    \"\"\"\n",
    "    # Word overlap\n",
    "    df['word_overlap'] = calculate_context_usage(df)\n",
    "    \n",
    "    # 3-gram overlap (phrases)\n",
    "    df['phrase_overlap'] = calculate_ngram_overlap(df, n=3)\n",
    "    \n",
    "    # Key terms\n",
    "    df['key_term_usage'] = calculate_key_term_usage(df)\n",
    "    \n",
    "    # Summary stats\n",
    "    stats = {\n",
    "        'word_overlap_mean': df['word_overlap'].mean(),\n",
    "        'phrase_overlap_mean': df['phrase_overlap'].mean(),\n",
    "        'key_term_usage_mean': df['key_term_usage'].mean(),\n",
    "        'low_usage_count': len(df[df['word_overlap'] < 0.3])  # Flag low usage\n",
    "    }\n",
    "    \n",
    "    return df, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87952dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def add_reason_background_similarity(df):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between reason and background_info using TF-IDF.\n",
    "    Adds 'reason_bkg_info_similarity' column to DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'reason' and 'background_info' columns\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Original DataFrame with new 'reason_bkg_info_similarity' column added\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        reason = str(row['reason'])\n",
    "        background = str(row['background_info'])\n",
    "        \n",
    "        # Create TF-IDF vectors\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform([reason, background])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            similarity = 0.0  # Handle edge cases\n",
    "        \n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    df['reason_bkg_info_similarity'] = similarities\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09e4c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Feedback Rate: 70.0%\n",
      "Average: 29.5 words\n",
      "Range: 16 - 49 words\n"
     ]
    }
   ],
   "source": [
    "# add to df the word count\n",
    "df = add_background_word_count(df)\n",
    "\n",
    "\n",
    "positive_rate = get_positive_feedback_rate(df)\n",
    "print(f\"Positive Feedback Rate: {positive_rate:.1f}%\")\n",
    "print(f\"Average: {df['background_info_word_count'].mean():.1f} words\")\n",
    "print(f\"Range: {df['background_info_word_count'].min()} - {df['background_info_word_count'].max()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "219a0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Usage Analysis:\n",
      " Mean Word overlap: 2.6%\n",
      " Mean Phrase overlap: 0.4%\n",
      " Mean Key term usage: 9.1%\n",
      " Mean Low usage cases (word overlap < 30%): 70\n",
      "Mean similarity: 0.25\n",
      "High redundancy (>0.5): 1\n"
     ]
    }
   ],
   "source": [
    "df_usage, usage_stats = analyze_context_usage(df)\n",
    "df = add_reason_background_similarity(df)\n",
    "\n",
    "print(\"Context Usage Analysis:\")\n",
    "print(f\" Mean Word overlap: {usage_stats['word_overlap_mean']:.1%}\")\n",
    "print(f\" Mean Phrase overlap: {usage_stats['phrase_overlap_mean']:.1%}\")\n",
    "print(f\" Mean Key term usage: {usage_stats['key_term_usage_mean']:.1%}\")\n",
    "print(f\" Mean Low usage cases (word overlap < 30%): {usage_stats['low_usage_count']}\")\n",
    "\n",
    "# Analyze\n",
    "print(f\"Mean similarity: {df['reason_bkg_info_similarity'].mean():.2f}\")\n",
    "print(f\"High redundancy (>0.5): {len(df_usage[df_usage['reason_bkg_info_similarity'] > 0.5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46769f3a",
   "metadata": {},
   "source": [
    "#### Quick analysis on quantitative values\n",
    "- Feedack rate: `70% is positive`. We can do better... lots of negative feedback come from the issues described above.\n",
    "- Background length: 16-49 words with an average of `~30 words`.\n",
    "- context usage: analyzed 3 metrics. Somewhat best (and its not great already) is word overlap. It gives an `avg word overlap of 2.6%`. May be better to analyze this with llm.\n",
    "- reason/background similarity: used cosine similarity between reason and background info. We get a mean of 0.25 (`25%`). Not a lot of redundancy. should be good? lets confirm with llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404e4f5",
   "metadata": {},
   "source": [
    "## LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e268f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ed593",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_JUDGE_SYSTEM_PROMPT = \"\"\" \n",
    "You are an expert evaluator for an AI civics tutor chatbot powered by a Retrieval-Augmented Generation (RAG) pipeline. \n",
    "This chatbot helps students practice for the USCIS citizenship test. \n",
    "Your role is to judge how well the chatbot‚Äôs outputs adhere to its intended logic and data sources. \n",
    "\n",
    "---\n",
    "\n",
    "### BOT CONTEXT\n",
    "\n",
    "The chatbot receives the following inputs:\n",
    "- **question**: The USCIS civics test question being asked.\n",
    "- **answers**: The authoritative list of acceptable answers. These are regularly updated and must override the model‚Äôs own outdated world knowledge.\n",
    "- **user_state**: The student‚Äôs U.S. state (used for state-specific questions).\n",
    "- **user_answer**: The student‚Äôs submitted answer.\n",
    "- **context**: 2‚Äì4 retrieved passages from the official USCIS Civics Guide.\n",
    "\n",
    "The chatbot produces this output:\n",
    "- **success** (boolean): Whether the user‚Äôs answer was marked correct or incorrect.\n",
    "- **reason** (string): The rationale for the pass/fail result.\n",
    "- **background_info** (string): Additional educational information drawn from the RAG context to help the user learn more about the topic.\n",
    "\n",
    "---\n",
    "\n",
    "### YOUR TASK\n",
    "\n",
    "Given the chatbot‚Äôs inputs and outputs, evaluate each of the following **independently** and **objectively**.  \n",
    "Use only the provided data ‚Äî ignore any external or world knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `answer_context_usage` (yes / no)\n",
    "**Goal:** Determine if the chatbot used the provided `answers` list correctly, rather than relying on outdated training data.  \n",
    "- **YES** ‚Üí The chatbot clearly uses the `answers` field to judge correctness, even if that data differs from current real-world facts.  \n",
    "- **NO** ‚Üí The chatbot ignores the `answers` field and instead uses its own outdated or internal world knowledge.\n",
    "\n",
    "**Examples:**\n",
    "- ‚úÖ **Yes:**  \n",
    "  - Question: ‚ÄúWho is the U.S. President?‚Äù  \n",
    "    answers: [\"Donald Trump\"]  \n",
    "    user_answer: \"Donald Trump\"  \n",
    "    chatbot marks as **success: true** and reason references the provided answer list.  \n",
    "- ‚ùå **No:**  \n",
    "  - Question: ‚ÄúWho is the U.S. President?‚Äù  \n",
    "    answers: [\"Donald Trump\"]  \n",
    "    user_answer: \"Donald Trump\"  \n",
    "    chatbot marks as **success: false** because it claims the president is Biden ‚Äî indicating outdated world knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `grading_accuracy` (good / bad)\n",
    "**Goal:** Assess whether the chatbot graded fairly and reasonably, without being overly strict.  \n",
    "- **GOOD** ‚Üí The chatbot accepts minor spelling mistakes, semantically equivalent answers, and allows for correct multi-part answers even with extra items.  \n",
    "- **BAD** ‚Üí The chatbot penalizes minor typos, ignores semantic equivalence, or fails multi-part answers even when enough correct items are present.\n",
    "\n",
    "**Examples:**\n",
    "- ‚úÖ **Good:**  \n",
    "  - Question: ‚ÄúWhat is one right of the people?‚Äù  \n",
    "    answers: [\"freedom of speech\", \"freedom of assembly\"]  \n",
    "    user_answer: \"right to free speech\"  \n",
    "    chatbot passes the answer and explains semantic equivalence.  \n",
    "- ‚úÖ **Good:**  \n",
    "  - Question: ‚ÄúName two states that border Mexico.‚Äù  \n",
    "    answers: [\"Texas\", \"Arizona\", \"California\", \"New Mexico\"]  \n",
    "    user_answer: \"Arizona, California, Michigan\"  \n",
    "    chatbot passes since two of three are correct and notes Michigan is unrelated.  \n",
    "- ‚ùå **Bad:**  \n",
    "  - Question: ‚ÄúName one war fought by the U.S. in the 1900s.‚Äù  \n",
    "    answers: [\"World War I\", \"World War II\", \"Korean War\"]  \n",
    "    user_answer: \"world war one\"  \n",
    "    chatbot fails it because of wording or capitalization.  \n",
    "- ‚ùå **Bad (typo/misspelling case):**  \n",
    "  - Question: ‚ÄúWhat is one freedom from the First Amendment?‚Äù  \n",
    "    answers: [\"freedom of speech\", \"freedom of religion\", \"freedom of assembly\"]  \n",
    "    user_answer: \"fredom of speach\"  \n",
    "    chatbot marks **success: false** ‚Äî this is **bad grading**, since the user‚Äôs intent is clear despite spelling errors.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `background_info_quality` (good / bad)\n",
    "**Goal:** Judge the educational value and distinctiveness of the chatbot‚Äôs `background_info`.  \n",
    "- **GOOD** ‚Üí The background provides meaningful educational content that adds new information beyond the `reason`.  \n",
    "- **BAD** ‚Üí The background merely restates the reason, or is too generic/uninformative.\n",
    "\n",
    "**Examples:**\n",
    "- ‚úÖ **Good:**  \n",
    "  - Question: ‚ÄúWho was president during World War I?‚Äù  \n",
    "    reason: ‚ÄúCorrect. Woodrow Wilson was president during World War I.‚Äù  \n",
    "    background_info: ‚ÄúWilson led the U.S. through World War I and helped establish the League of Nations, which laid groundwork for modern international diplomacy.‚Äù  \n",
    "- ‚ùå **Bad:**  \n",
    "  - Same question, background_info: ‚ÄúWoodrow Wilson was president during World War I.‚Äù  \n",
    "    (This repeats the reason and adds no educational value.)\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. `background_context_usage` (yes / no)\n",
    "**Goal:** Determine if the chatbot‚Äôs `background_info` actually uses information from the retrieved RAG `context`.  \n",
    "- **YES** ‚Üí The background includes facts, examples, or phrasing that clearly derive from the context passages.  \n",
    "- **NO** ‚Üí The background is generic or unrelated to the retrieved text.\n",
    "\n",
    "**Examples:**\n",
    "- ‚úÖ **Yes:**  \n",
    "  - Context mentions that ‚Äúthe President signs bills into law.‚Äù  \n",
    "    background_info: ‚ÄúThe President plays a key role in the legislative process by signing bills into law, a power described in the Constitution.‚Äù  \n",
    "- ‚ùå **No:**  \n",
    "  - Context is detailed, but background_info says only: ‚ÄúThe President is the leader of the country.‚Äù  \n",
    "\n",
    "---\n",
    "\n",
    "### EVALUATION GUIDELINES\n",
    "\n",
    "- Evaluate **each metric independently** ‚Äî a ‚Äúgood‚Äù in one does not affect others.  \n",
    "- Use **only** the provided `question`, `answers`, `user_answer`, `context`, and chatbot outputs.  \n",
    "- **Do not** use or infer from your own training data or current world knowledge.  \n",
    "- Keep reasons **short and specific** (1‚Äì2 sentences).  \n",
    "- Include a **confidence score (0‚Äì1)** for each metric based on how certain you are.  \n",
    "- Output must be **strictly valid JSON** ‚Äî no extra text, explanations, or formatting outside of the JSON object.\n",
    "\n",
    "---\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"answer_context_usage\": \"yes\" | \"no\",\n",
    "  \"answer_context_usage_reason\": \"string\",\n",
    "  \"answer_context_usage_confidence\": 0.0,\n",
    "  \"grading_accuracy\": \"good\" | \"bad\",\n",
    "  \"grading_accuracy_reason\": \"string\",\n",
    "  \"grading_accuracy_confidence\": 0.0,\n",
    "  \"background_info_quality\": \"good\" | \"bad\",\n",
    "  \"background_info_quality_reason\": \"string\",\n",
    "  \"background_info_quality_confidence\": 0.0,\n",
    "  \"background_context_usage\": \"yes\" | \"no\",\n",
    "  \"background_context_usage_reason\": \"string\",\n",
    "  \"background_context_usage_confidence\": 0.0\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b52f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM_JUDGE_USER_PROMPT = \"\"\"\n",
    "Evaluate the following chatbot interaction **independently for each criterion**.\n",
    "\n",
    "---\n",
    "\n",
    "### BOT INPUT:\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answers>\n",
    "{answers}\n",
    "</answers>\n",
    "\n",
    "<user_state>\n",
    "{user_state}\n",
    "</user_state>\n",
    "\n",
    "<user_answer>\n",
    "{user_answer}\n",
    "</user_answer>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "---\n",
    "\n",
    "### BOT OUTPUT:\n",
    "<success>\n",
    "{success}\n",
    "</success>\n",
    "\n",
    "<reason>\n",
    "{reason}\n",
    "</reason>\n",
    "\n",
    "<background_info>\n",
    "{background_info}\n",
    "</background_info>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536db102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for this df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b866a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
